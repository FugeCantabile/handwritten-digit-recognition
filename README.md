# 基于 NumPy 的手写数字识别

这是一个使用 Python 和 NumPy 从零开始构建的神经网络项目，用于识别著名的 MNIST 手写数字数据集。该项目涵盖了数据加载、预处理、模型构建、训练和结果可视化的完整流程，所有计算均未使用深度学习框架（如 TensorFlow 或 PyTorch）。

## ✨ 功能特性

  - **纯 NumPy 实现**: 整个神经网络的核心逻辑，包括前向传播、反向传播和梯度下降，都只使用 NumPy 实现。
  - **模块化设计**: 代码结构清晰，将数据处理、模型层、训练循环和可视化等功能解耦到不同模块中。
  - **数据预处理**: 支持对图像数据进行归一化（Normalization）和随机打乱（Shuffle），以提升模型性能和稳定性。
  - **多种网络层**: 实现了多种基础层，包括线性层（`Linear`）和多种激活函数层（`ReLU`, `Sigmoid`, `Tanh`）。
  - **训练与评估**: 包含完整的训练和测试循环，并在每个周期（epoch）结束后报告训练集和测试集的损失（Loss）与准确率（Accuracy）。
  - **可视化**: 训练结束后，能够自动绘制损失和准确率曲线图，并随机展示测试集中的样本、真实标签及模型的预测结果。
  - **模型持久化**: 支持将训练好的模型权重保存到文件，并能随时加载以进行推理或继续训练。

## 📂 项目结构

```
.
├── main.py                 # 项目主入口，整合所有模块并执行训练流程
├── model.py                # 定义神经网络的整体架构
├── layers.py               # 定义了各种神经网络层（线性层、激活函数层）
├── train.py                # 包含训练循环、损失函数和评估逻辑
├── dataset.py              # 负责加载和预处理 MNIST 数据集
├── IDX_reader.py           # 用于解析 MNIST 的 IDX 二进制文件格式
├── visualization.py        # 提供可视化功能（绘制曲线图、展示预测样本）
├── FullyConnectedLayer.py  # (辅助文件) 一个全连接层的独立实现示例
└── dataset/                # 存放 MNIST 数据集文件的目录
    ├── train-images-idx3-ubyte
    ├── train-labels-idx1-ubyte
    ├── t10k-images-idx3-ubyte
    └── t10k-labels-idx1-ubyte
```

## 🧠 模型架构

本项目中构建的模型 (`model.py`) 是一个包含三个线性层和两个ReLU激活函数、一个Sigmoid激活函数的四层前馈神经网络。

其结构如下：

1.  **输入层**: 784个神经元 (对应 28x28 像素的扁平化图像)
2.  **隐藏层 1**: `Linear(784, 64)` -\> `ReLU()`
3.  **隐藏层 2**: `Linear(64, 32)` -\> `ReLU()`
4.  **输出层**: `Linear(32, 10)` -\> `Sigmoid()` (输出10个类别，分别对应数字0-9的概率)

## 🔧 环境依赖

在运行此项目之前，请确保您已安装以下 Python 库：

  - `numpy`
  - `matplotlib`
  - `seaborn`

您可以使用 pip 来安装它们：

```bash
pip install numpy matplotlib seaborn
```

## 🚀 如何运行

1.  **下载数据集**: 从 [MNIST 官网](http://yann.lecun.com/exdb/mnist/) 下载以下四个文件：

      - `train-images-idx3-ubyte.gz`
      - `train-labels-idx1-ubyte.gz`
      - `t10k-images-idx3-ubyte.gz`
      - `t10k-labels-idx1-ubyte.gz`
        解压这些文件。

2.  **设置目录**: 在项目根目录下创建一个名为 `dataset` 的文件夹，并将解压后的四个文件放入其中。

3.  **运行主程序**: 在项目根目录下，执行 `main.py` 文件。

    ```bash
    python main.py
    ```

程序将开始训练，并在控制台输出每个周期的损失和准确率。训练结束后，会显示训练曲线图和样本预测结果，同时会在根目录下生成一个名为 `model.npz` 的文件，其中包含了训练好的模型权重。

## 📜 代码文件详解

#### `main.py`

项目的主脚本。它负责：

  - 设置数据集路径。
  - 初始化 `MNISTDataset` 来加载训练和测试数据。
  - 初始化 `Module` 模型。
  - 调用 `train` 函数开始训练，并获取历史记录。
  - 将训练好的模型参数保存到 `model.npz`。
  - 调用 `visualization.py` 中的函数来展示训练结果。

#### `dataset.py`

定义了 `MNISTDataset` 类，它是一个可迭代的数据加载器。

  - 使用 `IDX_reader` 读取原始数据。
  - 可选择对图像数据进行归一化处理。
  - 将标签（0-9）转换为 one-hot 编码。
  - 实现 `__iter__` 和 `__next__` 方法，支持按批次（batch）提供数据，并能在每个周期开始时随机打乱数据。

#### `IDX_reader.py`

一个实用工具模块，专门用于读取 MNIST 数据集独特的 `.idx-ubyte` 二进制文件格式。它使用 `struct` 模块来解析文件头和像素/标签数据。

#### `model.py`

定义了 `Module` 类，即神经网络模型本身。

  - 在 `__init__` 中，它将 `layers.py` 中定义的各个层（`Linear`, `ReLU`, `Sigmoid`）堆叠起来，构成一个完整的网络。
  - `forward()` 方法定义了数据从输入到输出的前向传播路径。
  - `backward()` 方法以相反的顺序调用每一层的反向传播。
  - `step()` 方法调用每一层的参数更新（梯度下降）。
  - `save()` 和 `load()` 方法用于模型的保存和加载。

#### `layers.py`

定义了网络的基本构建块。

  - **`Layer` (基类)**: 定义了所有层都应具备的接口 (`forward`, `backward`, `step`)。
  - **`Linear`**: 实现了一个全连接层。它包含权重 `w` 和偏置 `b`，并实现了其前向计算、梯度计算和参数更新的逻辑。
  - **`Sigmoid`, `ReLU`, `Tanh`**: 实现了激活函数层。它们没有可训练的参数，主要负责对输入数据进行非线性变换及其梯度的计算。

#### `train.py`

包含了模型训练的核心逻辑。

  - **`train()`**: 主训练函数，循环指定的周期（epochs）。在每个周期内，它遍历所有训练数据，执行前向传播、计算损失、反向传播和更新权重。同时，在每个周期结束后，它会在测试集上评估模型的性能。
  - **`square_loss()`**: 计算预测值和真实值之间的均方误差损失及其梯度。
  - **`is_right()`**: 计算一个批次中预测正确的样本数量。

#### `visualization.py`

提供了两个用于结果可视化的函数。

  - **`plot_training_curves()`**: 使用 Matplotlib 绘制训练过程中的损失和准确率曲线，便于分析模型的收敛情况和是否过拟合。
  - **`plot_sample_predictions()`**: 随机从测试集中抽取样本，显示图像并同时标注其真实标签和模型的预测标签，直观地展示模型的性能。

#### `FullyConnectedLayer.py`

这是一个独立的、功能更全面的全连接层实现，其中激活函数被整合到了层内部。在当前项目中，`main.py` 并未直接使用此文件，而是采用了 `layers.py` 中将线性和激活函数分离的模式。此文件可作为一个备用或参考实现。
